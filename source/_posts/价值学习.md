---
title: 价值学习—Q-Learning与DQN算法实现
date: 2024-03-20 15:39:08
tags: [Reinforcement Learning, 算法实现]
categories: Reinforcement Learning
---

# 价值学习 Value-Based Reinforcement Learning

<mark>***本文理论部分内容主要参考王树森老师的强化学习教程***</mark>：[王老师参考书-DRL](https://github.com/wangshusen/DRL/blob/master/Notes_CN/DRL.pdf)

## Value-based learning

### 预备知识

参考书籍的第一部分，至少了解强化学习中智能体(Agent)、策略(Policy)等概念。

### 价值函数

介绍价值函数前，先简要了解/回顾一下奖励(Reward)与回报(Return)。

- 奖励：当智能体执行一个动作后，环境会反馈给智能体一个数值，即奖励。奖励的大小一定程度上反映了这个动作的价值。一般假设奖励函数为$R_t(S_t,A_t,S_{t+1})$。
- 回报：即一局游戏从当前时刻到最后游戏结束，智能体收到的累计奖励。一般回报指的是折扣后的回报：$U_t=\sum_{k=t}^{n}\gamma^{k-t}\cdot R_k$。

价值函数则表示的是回报的期望，两种价值函数如下：

- **动作价值函数：** $\left.Q_\pi(s_t,a_t)\left.=\left.\mathbb{E}\right[U_t\right|S_t=s_t,A_t=a_t\right]$是使用当前策略$\pi$下，对当前”状态和动作对“的评分。当前动作$a_t$可以是任意动作空间内的动作，而未来的动作则服从$\pi$的分布。
- **最优动作价值函数：**$Q_\star(s_t,a_t)\text{ = max}_{\pi}~Q_\pi(s_t,a_t)$即是对动作价值取最大值，即无论策略$\pi$选什么，当前"状态动作对"能获得的期望回报$Q_\pi$都不会超过$Q_*$。

### 价值学习

价值学习的思想来源于最优动作价值函数：若我们能通过某种方法学习到最优动作价值函数$Q_*(S,A)$，那么对任意时刻的观测状态$s_t$，我们都能通过$\text{argmax}_{a\in\mathcal{A}}~Q_*(s_t,a)$来从动作空间中找到最佳的动作，从而最大化我们的期望回报。

值得一提的是，价值学习等价与我们学到到一个隐式策略：$\pi^*(\cdot|s_t)$，其分布是$P(a=\text{argmax}_{a\in\mathcal{A}}~Q_*(s_t,a))=1$，其余动作的概率为0。显然，这是一个确定策略 (输出的动作没有随机性)。

## Q-learning

假设观测到的状态和动作都是离散有限的，则很容易建如下一张表：

![image-20240312152011420](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/12/20240312-152011.png)

<center><p>截图来自王树森-《深度强化学习》</p></center>

每个单元格内存放着对应"动作状态对"的$Q_*$值。

### 如何获得Q-Table?

**最优贝尔曼方程** (**定理 4.1** Page.56)：
$$
Q_\star(s_t,a_t)~=~\mathbb{E}_{S_{t+1}\sim p(\cdot|s_t,a_t)}\Big[R_t+\gamma\cdot\max_{A\in\mathcal{A}}Q_\star(S_{t+1},A)\Big|S_t=s_t,A_t=a_t\Big].
$$
方程(1)实际上是一个递归方程，其建立了这一时刻$Q_*(s_t,a_t)$与下一时刻$Q_*(s_{t+1},a_{t+1})$的关系。利用这个关系，我们可以不断让表格从随机初始化状态朝着真实的$Q_*$更新。

**更新动作 (单步TD)：**

- 方程(1)左边近似为当前时刻表格中"动作状态对"$(s_t,a_t)$，假设在图 4.5中为 (第1种状态，第2种动作)，其值为$-95$；

- 执行动作后，环境会返回一个奖励，假设为$r_t=66$，以及新的状态，假设为$s_{t+1}=\text{第3种状态}$，则方程(1)右边的值应近似为$66+0.9\cdot413=437.7$。（因为第3种状态里最大Q值是413，并且假设折扣率$\gamma=0.9$）

- 按方程(1)，左右两边的近似值在理想状态应该相等，但显然$-95\ne437.7$。根据**TD算法**的思想，右边的值部分基于真实观测奖励$r_t$，具有更多的可靠性，所以我们要让左边（表格上的原值）变得更接近右边（计算的值）。

- 新的值计算公式为：$(1-\alpha)\cdot(-95)+\alpha\cdot437.7$，其中$\alpha<1$为学习率。假设$\alpha=0.1$，则表格原位置的值由$-95$更新为$-41.73$。

- 反复重复上述动作，表格中的值就会更加接近真实的Q值。

### Q-learning 算法实现

#### CartPole环境介绍

![CartPole.gif](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/13/20240313-162839.gif)

CartPole环境：<https://www.gymlibrary.dev/environments/classic_control/cart_pole/>

- Agent：小车cart
- Action：左右两个方向的固定大小的力

| Num | Action                 |
| --- | ---------------------- |
| 0   | Push cart to the left  |
| 1   | Push cart to the right |

- State/Observation：小车水平位置，小车速度，杆的垂直偏离角度，杆的角速度

| Num | Observation           | Min                 | Max               |
| --- | --------------------- | ------------------- | ----------------- |
| 0   | Cart Position         | -4.8                | 4.8               |
| 1   | Cart Velocity         | -Inf                | Inf               |
| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |
| 3   | Pole Angular Velocity | -Inf                | Inf               |

- Goal：通过对小车施加两个方向的力，尽可能让杆保持竖直
- Reward：每次执行动作后若没触发结束条件，则奖励+1
- 回合结束条件：（其中之一）
  - Termination: Pole Angle is greater than ±12°
  - Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)
  - Truncation: Episode length is greater than 500 (200 for v0) （实验发现，单回合步长超过了环境也并不会主动结束）

#### 注意事项

1. Q学习是一种**异策略**(Off-policy)算法，即用来收集**经验**(experience)的**行为策略**可以和我们的**目标策略**不同。正如前面所说，我们实际上学习的是一个确定策略，输出的动作没有随机性，而为了增加**探索**性，我们使用**$\epsilon$-greedy策略**来与环境交互，产生每一步的动作。

$\epsilon$-greedy策略定义如下：
$$
\text{action}=\left\{\begin{matrix}
\text{argmax}_{a\in\mathcal{A}}~Q_*(s_t,a),  &\text{random}(0,1)\ge \epsilon  \\
\text{random}(a\in\mathcal{A}),  &\text{random}(0,1)<\epsilon 
\end{matrix}\right.，
$$
其中$\epsilon<1$表示探索概率，并且随着训练轮数逐渐递减。

2. 观测空间是连续的，而Q-table只能处理有限的状态量，所以离散化状态空间是必要的。

#### 训练效果

每1000回合的平均episode累计奖励：

<img src="https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/13/20240313-174358.png" alt="q-learning1" style="zoom:100%;" />

加一点modification后效果：

<img src="https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/13/20240313-174450.png" alt="q-learning2" style="zoom:100%;" />

### 参考链接

- <https://medium.com/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5>

- <https://github.com/JackFurby/CartPole-v0/tree/master>

- <https://medium.com/swlh/using-q-learning-for-openais-cartpole-v1-4a216ef237df>

- [An Introduction to Q-Learning: A Tutorial For Beginners](https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial)

(代码链接在文末)

## Deep Q Network (DQN)

对观测的离散处理一方面能降低计算开销，另一方面不可避免地损失掉一些可靠性。DQN的思想则是用神经网络去近似我们的最优动作价值函数$Q_*(a,s)$，这样就能处理连续的状态输入了。

![image-20240315102943870](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/15/20240315-102944.png)

<center><p>截图来自王树森-《深度强化学习》</p></center>

### DQN训练方法

训练DQN一样使用TD算法。根据**定理 4.1**，我们总是希望左边的近似值（当前时间步的估计值）能趋于右边下一时刻的近似值：

- 初始化DQN网络$Q(s,a;w)$
- 获取一个更新用的transition: $(s_t,a_t,r_t,s_{t+1})$
  - 左边近似值：$Q(s_t,a_t,w)$
  - 右边近似值(TD target)：$\hat y_t = r_t+\gamma\cdot \max_{a\in\mathcal{A}}Q(s_{t+1},a;w)$
  - TD error: $\delta_t = Q(s_t,a_t,w)-\hat y_t$
  - 定义损失函数: $L(w)=\frac12\delta^2$
- 对损失函数求梯度，然后做梯度下降就能逐步减小左右值的差(如何确保是左边靠向右边：$\hat y_t$不计算梯度，只当做一个常值即可)

### DQN训练流程：

- 使用**经验回放** (experience  replay) 方法，预先用行为策略收集最多b组transition: $(s_t,a_{t+1},r_t,s_{t+1})$，采用mini-batch和SGD方法更新。

- 引入目标网络缓解**自举**(bootstrapping)带来的问题
1. 初始化经验回放数组(replay buffer)，每一次交互后存入四元组$(s_t,a_{t+1},r_t,s_{t+1})$
2. 每回合结束后，从回放数组中随机抽取minibatch：$s_t^\text{batch}, a_t^\text{batch},r_t^\text{batch},s_{t+1}^\text{batch}$
3. 用主网络计算(batch_size)n个$Q_*$的估计值: $\hat q_t = Q(s_t^\text{batch},a_t^\text{batch};w)$
4. 用目标网络计算n个TD target: $\hat y_t = r_t+\gamma\cdot \max_{a\in\mathcal{A}}\bar Q(s_{t+1},a;\bar w)$ **(不计算梯度)**
5. 计算损失函数: $L(w)=\frac{1}{2n}\sum_{i=1}^n(\hat q_t^i-\hat y_t^i)^2$ **(均方差MSE)**
6. 反向传播，梯度下降更新参数: $w\prime \leftarrow w-\alpha\cdot\delta\cdot \nabla_{w}Q(s_t,a_t;w)$
7. 延迟更新目标网络: $\bar w\prime \leftarrow w\prime$

### DQN算法实现

(代码链接在文末)

#### 构造Agent类

- `Replay_buffer` 经验回放池 
  - `add_experience`: 存放每次交互后获得的4元组$(s_t,a_t,r_t,s_{t+1})$以及回合结束判断: done
  - `get_batch`: 从数组中随机取出min-ibatch来更新
- `DQN` 深度Q网络
  - 定义网络结构
  - `forward`: 前向传播，获得输入状态下各动作分数
- `Agent`
  - 对象
    - 定义超参数：折扣率`Gamma`，学习率`learning_rate`
    - 回放池：`buffer`
    - 主网络：`main_net`
    - 目标网络：`target_net`
    - 损失函数、优化器：`loss`, `optimizer`
  - 函数
    - `get_action`：返回最大Q值对应的动作
    - `load_pretrained_model`：加载预训练模型
    - `save_trained_model`：保存模型参数

```python
# 经验回放池
class Replay_buffer:
    def __init__(self, n_s, n_a):
        self.n_s = n_s
        self.n_a = n_a
        self.BUFFER_SIZE = 10000
        self.BATCH_SIZE = 64
        self.t_buf = 0
        self.t_max = 0

        # 因为s r a每个大小不一样，先申请空间(空或者随机初始化)
        self.all_s = np.empty(shape=(self.BUFFER_SIZE, self.n_s), dtype=np.float32)
        self.all_a = np.random.randint(low=0, high=n_a, size=self.BUFFER_SIZE, dtype=np.uint8)
        self.all_r = np.empty(shape=self.BUFFER_SIZE, dtype=np.float32)
        self.all_done = np.random.randint(low=0, high=2, size=self.BUFFER_SIZE, dtype=np.uint8)
        self.all_s_ = np.empty(shape=(self.BUFFER_SIZE, self.n_s), dtype=np.float32)

    def add_experience(self, s, a, r, done, s_):
        self.all_s[self.t_buf] = s
        self.all_a[self.t_buf] = a
        self.all_r[self.t_buf] = r
        self.all_done[self.t_buf] = done
        self.all_s_[self.t_buf] = s_
        self.t_buf = (self.t_buf + 1) % self.BUFFER_SIZE # 既加1，又最大重置，替换掉前面旧的经验
        self.t_max = max(self.t_max, self.t_buf) # 一开始随t_buf逐渐增加，到t_buf重置后不再跟随并保持不变，用来检查经验池经验数量

    # opt +  加光标点击多处 同时输入
    def get_batch(self):
        # 存的经验大于batch时随机抽, 不够时有多少取多少
        if self.t_max >= self.BATCH_SIZE: # 
            indices = random.sample(range(self.t_max), self.BATCH_SIZE) # 从有效索引中随机取索引
        else:
            indices = range(0, self.t_max)

        batch_s = []
        batch_a = []
        batch_r = []
        batch_done = []
        batch_s_ = []

        for idx in indices:
            batch_s.append(self.all_s[idx])
            batch_a.append(self.all_a[idx])
            batch_r.append(self.all_r[idx])
            batch_done.append(self.all_done[idx])
            batch_s_.append(self.all_s_[idx])
        # 按住option复制光标，双击能选中变量一键复制
        batch_s_tensor = torch.as_tensor(np.asarray(batch_s), dtype=torch.float32)
        batch_a_tensor = torch.as_tensor(np.asarray(batch_a), dtype=torch.int64).unsqueeze(-1) # (2,) -> (2,1) batch_a作为index必须int64
        batch_r_tensor = torch.as_tensor(np.asarray(batch_r), dtype=torch.float32).unsqueeze(-1)
        batch_done_tensor = torch.as_tensor(np.asarray(batch_done), dtype=torch.float32).unsqueeze(-1)
        batch_s__tensor = torch.as_tensor(np.asarray(batch_s_), dtype=torch.float32)


        return batch_s_tensor, batch_a_tensor, batch_r_tensor, batch_done_tensor, batch_s__tensor

# 神经网络
class DQN(nn.Module):
    def __init__(self, n_input, n_output):
        super().__init__()

        self.net = nn.Sequential(
            nn.Linear(in_features=n_input, out_features=64),
            nn.Tanh(),
            nn.Linear(in_features=64, out_features=n_output)
        )

    def forward(self, x):
        return self.net(x)



class Agent:
    def __init__(self, n_input, n_output):
        self.n_input = n_input
        self.n_output = n_output
        #=====================#
        self.Gamma = 0.99
        self.learning_rate = 0.1
        #=====================#
        self.buffer = Replay_buffer(self.n_input, self.n_output) 

        self.main_net = DQN(self.n_input, self.n_output) 
        self.target_net = copy.deepcopy(self.main_net)

        self.loss = nn.functional.smooth_l1_loss
        self.optimizer = torch.optim.Adam(self.main_net.parameters(), lr=self.learning_rate)

    def get_action(self, obs):
        obs_tensor = torch.as_tensor(obs, dtype=torch.float32)
        q_value = self.main_net(obs_tensor).unsqueeze(0) # forward
        max_q_idx = torch.argmax(input=q_value) # 输出idx
        a_q_max = max_q_idx.item()
        return a_q_max

    def load_pretrained_model(self, model_path="./model/cartpole-dqn.pth"):
        self.main_net.load_state_dict(torch.load(model_path))

    def save_trained_model(self,model_parameters, model_path="./model/cartpole-dqn.pth"):
        torch.save(model_parameters, model_path)
```

#### 测试与训练

```python
# 用训练好的模型玩一局，pygame可视化
def test(agent):
    env_test = gym.make("CartPole-v1", render_mode="human")
    s, _ = env_test.reset()
    done = False
    while not done:
        a = agent.get_action(s)
        s, _, done, _, _ = env_test.step(a)
        env_test.render()
    env_test.close()


def train(env_train, input_dim, output_dim, is_test):
    #======================#
    epsilon_max = 1.0
    epsilon_min = 0.05
    epsilon_decay = 0.0005

    n_episode = 5000 # 5000正好，可以略降 或 设置max_step
    TARGET_UPDATE_FREQUENCY = 100
    #======================#
    agent = Agent(n_input=input_dim, n_output=output_dim)
    best_model_parameters = agent.main_net.state_dict()

    reward_array = np.empty(shape=n_episode) # 记录各回合的累计奖励
    avg_episode_reward = [] # 所有过往回合平均累计奖励 (每100回合统计一次)
    max_episode_reward = 0 # 最大单回合累计奖励
    for episode_i in tqdm(range(1, n_episode+1)):
        episode_reward = 0 # 单回合累计奖励
        s, _ = env_train.reset()
        done = False
        step_i = 0

        while not done: # 原来的for step_i in range(n_time_step):出现逻辑问题，手动break导致n_time_step实际并没有跑满，epsilon的实际降幅跨度较大
            step_i +=1
    # 1.根据epsilon-greedy策略选择行动
            # epsilon从1到0.05随episode_i指数衰减
            epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_decay * episode_i)
            int_random = random.random()
            if int_random <= epsilon:
                a = env_train.action_space.sample()
            else:
                a = agent.get_action(s) 
    # 2.执行行动获取观测值，储存到buffer
            s_, r, done, info, _ = env_train.step(a)
            agent.buffer.add_experience(s, a, r, done, s_) 
    # 3.后续工作
            s = s_
            episode_reward += r

            if done:
                reward_array[episode_i-1] = episode_reward
                # 保存累计奖励最多的模型的参数
                if episode_i >= 4000 and episode_reward > max(10000, max_episode_reward):
                    best_model_parameters = copy.deepcopy(agent.main_net.state_dict()) # 最后一次更新后因为没有跑过，不知道累计奖励，(可以考虑放test里)但影响不大
                    max_episode_reward = episode_reward

    # 4.从buffer抽样进行mini-batch训练（没有预存足够的experience）
        # 4.1 抽batch
        batch_s, batch_a, batch_r, batch_done, batch_s_ = agent.buffer.get_batch() 
        # 4.2 计算target
        with torch.no_grad(): # 不加不影响结果，optimi不含targetNN的参数
            target_q_values = agent.target_net(batch_s_) 

        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]
        ''' 操作解释
        [[s_3]             [[q3(a1), q3(a2)]               [[max(q3)]
        [s_1]             [q1(a1), q2(a2)]                 [max(q2)]
        ...     ->(Q_net)   ...                 ->(max)    ...
        [s_7]]            [q7(a1), q7(a2)]]                [max(q7)]]
        '''

        # 另一种方法: DDQN, 选择a*与计算maxq分开, 以缓解最大化带来的的高估, 但效果不佳, 可能需要额外调参(?)
        '''
        q_values_next = agent.main_net(batch_s_)
        index_a_max = torch.argmax(q_values_next, axis=1, keepdim=True) # 最大Q对应的动作a* (列)
        max_target_q_values = target_q_values.gather(1, index_a_max) # 按a*取Q_target
        '''

        # (1-batch_done): 若s_为last step，y_target = r 
        target_values = batch_r + agent.Gamma * (1-batch_done) * max_target_q_values 
        # 4.3 计算q_t
        q_values = agent.main_net(batch_s)
        a_q_values = torch.gather(input=q_values, dim=1, index=batch_a) # 计算是计算所有a对应的q，取只取batch_a对应的q(s,a)
        # 4.4 计算损失
        loss = agent.loss(target_values, a_q_values) # 类似L1 Loss的函数，默认返回mean(batch_loss)
        # 4.5 更新参数
        agent.optimizer.zero_grad() 
        loss.backward()
        agent.optimizer.step()

    # 5.更新target_net
        if episode_i % TARGET_UPDATE_FREQUENCY == 0:
            agent.target_net.load_state_dict(agent.main_net.state_dict())

            # 6. 打印统计量
            avg_last100_reward = np.mean(reward_array[episode_i-TARGET_UPDATE_FREQUENCY:episode_i]) # 最近100回合的平均累计奖励
            avg_episode_reward.append(np.mean(reward_array[:episode_i-1]))
            print("Episode: {},\tAvg.{} Reward: {:.2f},\tAvg.all Reward: {:.2f}".format(episode_i, TARGET_UPDATE_FREQUENCY, 
                  avg_last100_reward,
                  avg_episode_reward[-1]))

    agent.save_trained_model(best_model_parameters) # 保存模型参数
    print("Training is over! The best episode reward is {}".format(max_episode_reward))
    env_train.close()

    # 7. 画图
    plt.plot(range(1, n_episode+1, TARGET_UPDATE_FREQUENCY), avg_episode_reward)
    plt.xlabel('Episode')
    plt.ylabel('Avg. Reward')
    plt.savefig('./Figures/DQN_cartpole.png') # 图片保存路径
    plt.show()

    # 8. 测试
    if is_test:
        test(agent)
```

#### 运行

```python
# 可以选择从本地加载预训练好的模型 或 从0训练
if __name__ == "__main__":
    env = gym.make('CartPole-v1')
    input_dim = env.observation_space.shape[0]
    output_dim = env.action_space.n
    #==========================#
    is_load_model = False # 训练模式
    # is_load_model = True # 注释则训练，取消注释则加载已有模型

    is_test = True
    #==========================#

    if is_load_model: # 是否从指定路径中加载模型参数
        agent = Agent(n_input=input_dim, n_output=output_dim)
        env.close()
        agent.load_pretrained_model()
        if is_test:
            test(agent)

    else:
        train(env, input_dim, output_dim, is_test)
```

#### 训练结果

- 每100回合统计过去所有回合累计的平均奖励（整体上升）

<img src="https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/20/20240320-154000.png" alt="DQN_cartpole" style="zoom:100%;" />

- 每100回合统计最新100回合累计的平均奖励（多数情况曲线上下浮动大）

<img src="https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/17/20240317-130919.png" alt="DQN" style="zoom:100%;" />

用保存的模型测试基本可以维持杆不倒：(训练累计奖励为1353606)

![DQN_CartPole_result](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/20/20240320-154000.gif)

Human-level control through deep reinforcement learning[1]

![image-20240315123405306](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/20/20240320-223904.png)

<center><p>Pseudo code from paper[1]</p></center>

### 参考文献/链接

- [1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. A. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, D. Hassabis. Human-level control through deep reinforcement learning. Nature, 2015. 518: 529~533
- [2] <https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435>
- [3] <https://github.com/Dylan2020THU/dqn_cartpole/tree/main>
- [4] <https://github.com/mahakal001/reinforcement-learning/tree/master/cartpole-dqn>
- [5] <https://blog.csdn.net/ice_bear221/article/details/123735643>

完整代码链接：<https://github.com/xiyanzzz/RL-Implement/tree/main/Value-based>
