---
title: 策略学习—REINFORCE与AC算法实现
date: 2024-03-20 15:45:22
tags: [Reinforcement Learning, 算法实现]
categories: Reinforcement Learning

---

# 策略学习 Policy-Based Reinforcement Learning

<mark>***本文理论部分内容主要参考王树森老师的强化学习教程***</mark>：[王老师参考书-DRL](https://github.com/wangshusen/DRL/blob/master/Notes_CN/DRL.pdf)

## Policy Gradient

策略学习的目的是直接训练出一个策略$\pi(a|s;\theta)$，使其能根据当前观测到的状态(策略网络的输入)，输出动作空间中各动作的概率密度。所以，动作空间必须是离散有限的。

![image-20240307180708319](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/07/20240307-180708.png)

<center><p>截图来自王树森-《深度强化学习》</p></center>

- **奖励**(reward)，是智能体(agent)与环境交互中从环境中获得的激励，其函数关系一般假设为$R_t(S_t,A_t,S_{t+1})$。

- **折扣回报**定义如下: $U_t=\sum_{k=t}^{n}\gamma^{k-t}\cdot R_k$，表示从当前时间步到一回合(episode)结束累计获得的奖励，一般简称为回报。

一个好的决策，始终是为了能最大化期望回报。下面简单介绍一下几种”期望回报“：

- **动作价值函数：** $\left.Q_\pi(s_t,a_t)\left.=\left.\mathbb{E}\right[U_t\right|S_t=s_t,A_t=a_t\right]$是使用当前策略$\pi$下，对当前”状态和动作对“的评分，而当前动作$a_t$可以是任意动作空间内的动作。价值学习中，我们可以比较当前状态下所有动作的打分来选取最好的动作，以获得最高的期望回报。

- **状态价值函数：**$V_\pi(s_t)~=~\mathbb{E}_{A_t\sim\pi(\cdot|s_t;\boldsymbol{\theta})}\Big[Q_\pi(s_t,A_t)\Big]$是衡量使用当前策略$\pi$下，所在(具体观测到的)状态$s_t$的好坏，即所能获得的期望回报。状态价值与动作价值的区别是当前动作$a_t$是否服从我们的策略$\pi$。它即反映状态的好坏，也一定程度上反映我们的策略$\pi$的好坏。

对状态价值函数做期望，消除掉具体的$s_t$，获得策略梯度方法的目标函数(Objective function)。
$$
J(\boldsymbol{\theta})=\mathbb{E}_S\Big[V_\pi(S)\Big].
$$
该函数仍表示所用策略$\pi$能获得的期望回报，但用状态的概率分布消除了状态$S$带来的不确定性，以获得针对所有可能状态而言的回报。（我们并不知道环境的状态分布是怎么样的，所以这仅是理论值）

自然而然地，策略学习可以描述为这样一个优化问题: 
$$
\max_\theta J(\boldsymbol{\theta}).
$$
优化$\theta\prime\leftarrow\theta+\frac{\partial}{\partial\theta}J(\theta)$，自然要梯度，下面就是求$J(\theta)$的梯度了。

<center><p>。。。(亿些推导)</p></center>

**引理 7.3. 策略梯度的连加形式** (Page.111)

$$
\begin{aligned}
\frac{\partial J(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}}~&=~ \mathbb{E}_{S_1,A_1}\left[\boldsymbol{g}(S_1,A_1;\boldsymbol{\theta})\right] \\
&+~\gamma\cdot\mathbb{E}_{S_1,A_1,S_2,A_2}\left[\boldsymbol{g}(S_2,A_2;\boldsymbol{\theta})\right] \\
&+~\ldots \\
&+~\gamma^{n-1}\cdot\mathbb{E}_{S_1,A_1,S_2,A_2,S_3,A_3,\cdotp\cdotp S_n,A_n}\left[\boldsymbol{g}(S_n,A_n;\boldsymbol{\theta})\right]\\
&=\mathbb{E}_{S\sim d(\cdot)}\left[\mathbb{E}_{A\sim\pi(\cdot|S;\boldsymbol{\theta})}\left[\sum_{t=1}^n\gamma^{t-1}\cdot \boldsymbol{g}(S_t,A_t;\boldsymbol{\theta})\right]\right].
\end{aligned}
$$

**定理 7.5. 策略梯度定理(严谨的表述)** (Page.112，假设$S\sim d(\cdot)$为马尔科夫链稳态分布的概率质量(密度)函数)

$$
\frac{\partial J(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}}=\left.\sum_{t=1}^n\gamma^{t-1}\cdot\mathbb{E}_{S\sim d(\sim)}\left[\mathbb{E}_{A\sim\pi(\cdot|S,\boldsymbol{\theta})}\left[\boldsymbol{g}(S_t,A_t;\boldsymbol{\theta})\right]\right].\right.
$$

其中，$\boldsymbol{g}(s,a;\boldsymbol{\theta})\triangleq Q_\pi(s,a)\cdot\frac{\partial\left.\ln\pi(a|s;\boldsymbol{\theta})\right.}{\partial\boldsymbol{\theta}}$。

以上只是理论值，只存在于理论之中，真正优化计算中用到的梯度仅是近似。

1. 蒙特卡洛近似去掉两个期望：用随机观测值$s_t, a_t$带入$g(s,a;\theta)$中计算出结果，来代替$g$的期望值
2. $Q_\pi(s,a)$的近似：很遗憾，这个期望回报我们还是不知道怎么计算，只能再近似处理了
   - **REINFORCE:** 简单粗暴，打完一局游戏记录下所有的$r_k$，用真实的回报$u_t~=~\sum_{k=t}^n\gamma^{k-t}\cdot r_k$来近似回报的期望$Q_\pi(s_t,a_t)=\mathbb{E}[U_t]$。
   - **Actor-Critic:** 用”万能的“神经网络来近似$Q_\pi(s_t,a_t)$。

## REINFORCE

根据**引理 7.3**，经过两次近似，策略梯度可近似为如下随机梯度 (无偏估计 )：

$$
\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta})\quad\approx\quad\sum_{t=1}^n\gamma^{t-1}\cdot u_t\cdot\nabla_{\boldsymbol{\theta}}\ln\pi(a_t|s_t;\boldsymbol{\theta}).
$$

<mark>记 $\hat {\boldsymbol{g}}(s_t,a_t;\boldsymbol{\theta})\triangleq u_t\cdot\nabla_{\boldsymbol{\theta}}\ln\pi(a_t|s_t;\boldsymbol{\theta}).$</mark>

**训练流程：**

1. 用当前的策略$\pi(a|s;\theta)$玩一局 ($a_t$根据输出的概率质量函数作随机采样)，记录下一条轨迹：$s_1,a_1,r_1,~...,~s_n,a_n,r_n$.
2. 计算n个回报：$u_t~=~\sum_{k=t}^n\gamma^{k-t}\cdot r_k,\quad\forall~t=1,\cdots,n.$ **(无梯度计算)**
3. 利用收集到的n对$(s_t,~a_t)$，计算n个等 $loss_t=-\gamma^{t-1} \cdot u_t\cdot \ln\pi(a_t|s_t;\boldsymbol{\theta})$，求和 `.sum()`$\to L(\theta)$
4. 反向传播获得梯度：`L.backward()`
5. 用近似随机梯度做更新：
$$
\theta \prime \leftarrow \theta + \beta \cdot  \sum_{t=1}^n \gamma^{t-1} \cdot  u_t \cdot \nabla_{\boldsymbol{\theta} } \ln \pi(a_t|s_t; \boldsymbol{ \theta} ). $$

![image-20240309200251060](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/09/20240309-200251.png)

<center><p>Adapted from Sutton & Barto 2018, Reinforcement Learning- An Introduction</p></center>

在一些方法中，等效的损失函数是对**归一化**（normalized）后的n个$u_t\cdot \ln\pi(a_t|s_t;\boldsymbol{\theta})$求和

实现参考: <https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63>

> *“In practice it can can also be important to normalize these. For example, suppose we compute [discounted cumulative reward] for all of the 20,000 actions in the batch of 100 Pong game rollouts above. One good idea is to “standardize” these returns (e.g. subtract mean, divide by standard deviation) before we plug them into backprop. This way we’re always encouraging and discouraging roughly half of the performed actions. Mathematically you can also interpret these tricks as a way of controlling the variance of the policy gradient estimator. A more in-depth exploration can be found* [here](http://arxiv.org/abs/1506.02438)*.”*

### REINFORCE with baseline

$$
\frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}=\mathbb{E}_{S\sim d(\cdot)}\left[\mathbb{E}_{A\sim\pi(\cdot|S;\boldsymbol{\theta})}\left[\sum_{t=1}^n\gamma^{t-1}\cdot (Q_\pi(s,a)-{\color{Red}{b}} )\cdot\frac{\partial\left.\ln\pi(a|s;\boldsymbol{\theta})\right.}{\partial\boldsymbol{\theta}}\right]\right].
$$

设b为任意不依赖动作A的函数，称为动作价值函数$Q_\pi$的基线 (baseline)。可以证明，经过两次期望，含baseline的项变为0，数学上不影响原来的梯度值，但能有效减少近似梯度的方差。

一般，我们使用状态价值$V_\pi(s)$作为基线，并在实际计算中用价值网络$v(s;\omega)$来实现近似其值。

<mark>记 $\hat {\boldsymbol{g}}(s_t,a_t;\boldsymbol{\theta})\triangleq \left[u_t-v(s_t;w)\right]\cdot\nabla_{\boldsymbol{\theta}}\ln\pi(a_t|s_t;\boldsymbol{\theta}).$</mark>

**训练流程：**

1. 用当前的策略$\pi(a|s;\theta)$玩一局 ($a_t$根据输出的概率质量函数作随机采样)，记录下一条轨迹：$s_1,a_1,r_1,~...,~s_n,a_n,r_n$.

2. 动作价值$Q_\pi$的近似:
   
   - 计算n个回报：$u_t~=~\sum_{k=t}^n\gamma^{k-t}\cdot r_k,\quad\forall~t=1,\cdots,n.$ **(无梯度计算)**

3. 基线$V_\pi$的近似:
   
   - 预测n个价值：$\widehat{v_t}~=~v(s_t;\boldsymbol{w}),\quad\quad\forall~t=1,\cdots,n.$

4. 更新价值网络$v(s;w)$:
   
   - 计算n个误差：$\delta_t=\hat v_t-u_t$，使用均方误差作为损失函数：$L(\boldsymbol{w})=\frac1{2n}\sum_{t=1}^n\delta_t^2.$ **(`MSELoss(v,u)`)**
   
   - 对$L(\omega)$做反向传播，获得梯度: $\nabla_{\boldsymbol{w}} L(\boldsymbol{w})=\frac1n\sum_{t=1}^n\left(\widehat{v}_t-u_t\right)\cdot\nabla_{\boldsymbol{w}}v(s_t;\boldsymbol{w}).$
   
   - 更新价值网络：$\boldsymbol{w}\prime\leftarrow\boldsymbol{w}-\alpha\cdot\nabla_{\boldsymbol{w}}L(\boldsymbol{w}).$

5. 更新策略网络$\pi(a|s;\theta)$:
   
   - 计算等价损失函数：n个 $loss_t=\gamma^{t-1}\cdot \delta_t \cdot \ln\pi(a_t|s_t;\boldsymbol{\theta})$，**($\delta_t$`.detach()仅作为常值`)**，再求和 `.sum()`$\to L(\theta)$
   - 随机梯度**下降**更新策略网络：
$$
\theta\prime\leftarrow\theta-\beta\cdot \sum_{t=1}^n\gamma^{t-1}\cdot \delta_t\cdot\nabla_{\boldsymbol{\theta}}\ln\pi(a_t|s_t;\boldsymbol{\theta}). 
$$

![image-20240309203603824](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/09/20240309-203604.png)

<center><p>Adapted from Sutton & Barto 2018, Reinforcement Learning- An Introduction</p></center>

### REINFORCE with baseline算法实现 (Pytorch, CartPole-v1)

（代码链接在文末）

#### 构造Agent-类 （部分）

```python
class PolicyValueNN(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_layer = 256):
        super(PolicyValueNN, self).__init__()
        self.policyNN = nn.Sequential(
            nn.Linear(input_dim, hidden_layer),
            nn.ReLU(),
            nn.Linear(hidden_layer, output_dim),
            nn.Softmax(dim=-1)
        )
        self.valueNN = nn.Sequential(
            nn.Linear(input_dim, hidden_layer),
            nn.ReLU(),
            nn.Linear(hidden_layer, 1)
        )
    def forward(self, state):
        value = self.valueNN(state)
        dist = Categorical(self.policyNN(state))
        return dist, value


class Agent:
    def __init__(self, input_dim, output_dim, device):
        self.device = device
        #==========================#
        self.gamma = torch.tensor(0.99).float().to(self.device)
        self.lr = torch.tensor(5e-4).float().to(self.device)
        #==========================#
        self.model = PolicyValueNN(input_dim, output_dim).to(self.device)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)
```

#### 测试与训练函数 （部分）

```python
def train(env_train, input_dim, output_dim, is_test, device):

    agent = Agent(input_dim=input_dim, output_dim=output_dim, device=device)
    #======================#
    n_episodes = 2000
    PRINT_FREQUENCY = 20
    #======================#

    reward_episode_list = [] # 记录每回合累计奖励
    reward_ep20avg_list = [] # 记录最近20回合平均累计奖励
    max_episode_reward = 0 # 训练中单回合最大累计奖励
    best_model_parameters = copy.deepcopy(agent.model.state_dict())
    for episode_i in range(1, n_episodes+1):
        s, _ = env_train.reset()
        done = False
        step_i = 0

        log_prob_list = []
        value_list = []
        reward_list = []
        return_list = []

        while not done:
            step_i += 1
            # 前向，获取动作与状态价值
            dist, value = agent.model(torch.as_tensor(s, dtype=torch.float32).to(device))
            a = dist.sample() # <Categorical>对象，dist拥有很多<Attributes> 如dist.probs查看概率 / a为tensor([])

            # interact
            s_, r, done, _ , _ = env_train.step(a.item())

            # collect
            log_prob = dist.log_prob(a).unsqueeze(-1) # 等效 torch.log(dist.probs[a]), 单独对a的概率取对数(计算梯度), tensor标量转[]

            log_prob_list.append(log_prob.unsqueeze(-1))
            value_list.append(value.unsqueeze(-1))
            reward_list.append(torch.tensor([r],dtype=torch.float).unsqueeze(-1).to(device))

            s = s_

        # 反向计算回报
        return_u = 0
        for t in reversed(range(len(reward_list))):
            return_u = reward_list[t] + agent.gamma * return_u
            return_list.insert(0,return_u)

        log_prob_list = [(agent.gamma**i) * num for i,num in enumerate(log_prob_list)] # 为每项乘折扣 gamma^(t-1)

        # 将列表转为tensor([[],...,[]])
        log_prob_list = torch.cat(log_prob_list)
        value_list = torch.cat(value_list)
        return_list = torch.cat(return_list)
        reward_list = torch.cat(reward_list)

        # 更新
        delta_list = return_list - value_list # 含梯度
        policy_loss = -(log_prob_list * delta_list.detach()).sum() # 折扣后的“加权”求和 (其他思路(不折扣)：1. 对n个乘积求均值；2. 对n个乘积先归一化后求和)
        value_loss = 0.5 * delta_list.pow(2).mean()

        loss = policy_loss + 3 * value_loss # 相当于增大value网络的学习率
        agent.optimizer.zero_grad()
        loss.backward()
        agent.optimizer.step()
```

#### 运行

```python
# 可以选择从本地加载预训练好的模型 或 从0训练
if __name__ == "__main__":
    env = gym.make('CartPole-v1')
    input_dim = env.observation_space.shape[0]
    output_dim = env.action_space.n
    #==========================#
    is_load_model = False # 训练模式
    # is_load_model = True # 注释则训练，取消注释则加载已有模型

    is_test = True
    #==========================#

    if is_load_model: # 是否从指定路径中加载模型参数
        agent = Agent(input_dim, output_dim, device=device)
        env.close()
        agent.load_pretrained_model(model_path="./Models/cartpole-REINFb-56937.pth") # 数字部分表示训练分数
        if is_test:
            test(agent, device=device, is_sample = True)

    else:
        train(env, input_dim, output_dim, is_test, device=device)
```

#### 训练结果

- 每20回合对最近20回合，以及对过去所有回合累计奖励求平均

<img src="https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/18/20240318-221225.png" alt="REINFORCE_with_b4" style="zoom:100%;" />

- 保存模型测试 （训练得分56937）

![CartPole_v1_result_REINF](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/18/20240318-230528.gif)

能较长时间下不倒，但因为动作采样有随机性，还是会使小车到达边界而终止；若取消采样改用概率最大的动作，则能稳定不倒。

## Actor-Critic

根据**定理 7.5**，经过两次近似，策略梯度可以近似为如下随机梯度：
$$
\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta})\quad\approx\quad \sum_{t=1}^n\gamma^{t-1}\cdot q(s,a;\boldsymbol{w})\cdot\nabla_{\boldsymbol{\theta}}\ln\pi(a\mid s;\boldsymbol{\theta})
$$
其中，第一项常数项可省略 (被学习率吸收)。<mark>记 $\hat {\boldsymbol{g}}(s_t,a_t;\boldsymbol{\theta})\triangleq q(s_t,a_t;w)\cdot\nabla_{\boldsymbol{\theta}}\ln\pi(a_t|s_t;\boldsymbol{\theta}).$</mark>

![image-20240307225632805](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/07/20240307-225633.png)

<center><p>截图来自王树森-《深度强化学习》</p></center>

**训练流程（使用目标网络缓解自举）：**

1. 交互：
   
   - 观测当前状态$s_t$，依照当前策略$\pi(a|s;\theta)$做出随机决策$a_t$并执行。
   
   - 从环境中观测到奖励$r_t$和新的状态$s_{t+1}$。
   
   - 根据$s_{t+1}$，继续依照当前策略随机采样一个动作$\hat a_{t+1}$但**不执行**。

2. 更新价值网络$q(s,a;w)$：
   
   - 价值网络给$(s_t,a_t)$打分：$\hat q_t=q(s_t,a_t;\omega)$。
   
   - 目标网络给$(s_{t+1},\hat a_{t+1})$打分：$\bar{\hat q}_{t+1}=\bar q(s_{t+1},\hat a_{t+1};\bar \omega)$。**(`.detach()`仅作为常数参与到价值网络的更新)**
   
   - 计算TD误差：$\hat y_t=r_t + \gamma\cdot \bar{\hat q}_{t+1}$，$\delta_t=\hat q_t - \hat y_t$
   
   - 计算价值网络损失函数：$L(\omega)=\frac{1}{2}\delta_t^2$ 。**（标量）**
   
   - 反向传播，梯度下降更新价值网络：$\boldsymbol{w\prime}\leftarrow\boldsymbol{w}-\alpha\cdot\delta_t\cdot\nabla_{\boldsymbol{w}}q(s_t,a_t;\boldsymbol{w}).$

3. 更新策略网络$\pi(s|a;\theta)$：
   
   - 计算策略网络等价损失函数：$L(\theta)=-\hat q_t\cdot \ln \pi(a_t|s_t;\theta)$，其中$\delta_t$要`.detach()`，作为常数参与更新

4. 反向传播，梯度下降更新策略网络：$\boldsymbol{\theta\prime}\leftarrow\boldsymbol{\theta}+\beta\cdot\widehat{q_t}\cdot\nabla_{\boldsymbol{\theta}}\ln\pi(a_t\mid s_t;\boldsymbol{\theta}).$

5. 更新目标网络，加权平均：$\bar \omega \prime\leftarrow \tau\cdot w\prime + (1-\tau)\bar \omega.$

### Advantage Actor-Critic (A2C)

同样，对策略梯度添加baseline后：

$$
\begin{array}{rcl}\boldsymbol{g}(s,a;\boldsymbol{\theta})&=&[Q_{\pi}(s,a)-V_{\pi}(s)] \end{array} \cdot \nabla _ { \boldsymbol { \theta }}\ln\pi(a\mid s;\boldsymbol{\theta}).
$$

其中，第一项{$Q_{\pi}(s,a)-V_{\pi}(s)$}称作优势函数 (advantage function)。

根据贝尔曼最优化公式(**定理 A.2**, Page.303)：$Q_\pi(s_t,a_t)~=~\mathbb{E}_{S_{t+1}\sim p(\cdot|s_t,a_t)}\Big[~R_t~+~\gamma\cdot V_\pi(S_{t+1})~\Big]$，优势函数可表示为：
$$
\mathbb{E}_{S_{t+1}}\left[\left[R_t +\gamma \cdot V_\pi(S_{t+1})\right ]-V_{\pi}(s)\right]
$$
用价值网络$v(s;w)$近似状态价值函数$V_\pi(s)$后，<mark>近似策略梯度可表示为：</mark>
$$
\hat g(s_t,a_t;\theta)=[r_t+\gamma\cdot v(s_{t+1};w)-v(s_t;w)]\cdot \nabla _ { \boldsymbol { \theta }}\ln\pi(a_t\mid s_t;\boldsymbol{\theta})
$$

**网络：**

- 价值网络：$v(s;\omega)$，更新：$\boldsymbol{w}\leftarrow\boldsymbol{w}-\alpha\cdot\delta_t\cdot\nabla_{\boldsymbol{w}}v(s_t;\boldsymbol{w})$，其中$\delta_t=v(s_t;w)-[r_t+\gamma\cdot v(s_{t+1};w)]$为TD误差，损失函数为：$\left.L(\boldsymbol{w})\triangleq\left.\frac12\right[v(s_t;\boldsymbol{w})-\widehat{y}_t\right]^2$。
- 策略网络：$\pi(a|s;\theta)$，更新：$\boldsymbol{\theta}\leftarrow\boldsymbol{\theta}+\beta\cdot\widetilde{\boldsymbol{g}}(s_t,a_t;\boldsymbol{\theta})$，其中$\tilde g(s_t,a_t;\theta)=-\delta_t\cdot \nabla _ { \boldsymbol { \theta }}\ln\pi(a_t\mid s_t;\boldsymbol{\theta})$，等效GD损失函数为：$\delta_t\cdot \ln\pi(a_t\mid s_t;\boldsymbol{\theta})$，其中$\delta_t$仅作为常量参与更新，不计算参数。

**训练流程 (引用目标网络缓解自举):**

1. 交互：
   - 观测当前状态$s_t$，依照当前策略$\pi(a|s;\theta)$做出随机决策$a_t$并执行。
   - 从环境中观测到奖励$r_t$和新的状态$s_{t+1}$。
2. 获得TD误差(优势)：
   - 价值网络给$s_t$打分：$\hat v_t=v(s_t;\omega)$
   - 目标网络给$s_{t+1}$打分：$\hat v_{t+1}=\bar v(s_{t+1};\bar\omega)$ **(不计算梯度，仅作为常值)**
   - TD目标：$\hat y_t=r_t+\gamma\cdot \hat v_{t+1}$
   - TD误差：$\delta_t=\hat v_t-\hat y_t$
3. 更新价值网络：
   - 计算损失：$L(\omega)=\frac12\delta_t^2$ **(计算梯度)**
   - 反向传播，梯度下降更新参数：$\boldsymbol{w}\leftarrow\boldsymbol{w}-\alpha\cdot\delta_t\cdot\nabla_{\boldsymbol{w}}v(s_t;\boldsymbol{w})$
4. 更新策略网络：
   - 计算等价GD损失：$L(\theta)=\delta_t\cdot \ln\pi(a_t\mid s_t;\boldsymbol{\theta})$ **($\delta_t$仅作为常值，不参与梯度计算)**
   - 反向传播，梯度下降更新参数：$\boldsymbol{\theta\prime}\leftarrow\boldsymbol{\theta}-\beta\cdot \delta_t\cdot \nabla _ { \boldsymbol { \theta }}\ln\pi(a_t\mid s_t;\boldsymbol{\theta})$
5. 更新目标网络：
   - 加权平均：$\bar \omega \prime\leftarrow \tau\cdot w\prime + (1-\tau)\bar \omega$

![image-20240311163038184](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/11/20240311-163038.png)

<center><p>Adapted from Sutton & Barto 2018, Reinforcement Learning- An Introduction</p></center>

### A2C算法的实现 (Pytorch, CartPole-v1)

1. 在上述的A2C训练流程中，每一步(step)都进行了更新，即价值网络和策略网络的每次更新仅用到了$(s_t,a_t,r_t,s_{t+1})$。而在许多算法实现中，则是用当前策略$\pi$来完成一局(episode)游戏，记录下一条完整的轨迹$s_1,a_1,r_1,~...,~s_n,a_n,r_n$，并计算所有step的损失，用均值或和来进行梯度下降更新参数。
   
   - https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f
   - [CSDN Actor-Critic及Advantage Actor-Critic(A2C)原理及实战讲解](https://blog.csdn.net/M3197783956/article/details/135014753?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EYuanLiJiHua%7EPosition-2-135014753-blog-127143122.235%5Ev43%5Econtrol&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EYuanLiJiHua%7EPosition-2-135014753-blog-127143122.235%5Ev43%5Econtrol&utm_relevant_index=5)

2. 在下面给出实现算法中，使用的是Multi-step TD target。
   
   - TD target：$\hat y_t = \sum^{m-1}_{i=0}\gamma^i+\gamma^m\cdot v(s_{t+m};w)$.

（代码链接在文末）

#### 构造Agent-类 （部分）

```python
class ActorNN(nn.Module):
    def __init__(self, input_dim, output_dim, hiden_layer = 256):
        super(ActorNN, self).__init__()
        self.actorNN = nn.Sequential(
            nn.Linear(input_dim, hiden_layer),
            nn.ReLU(),
            nn.Linear(hiden_layer, output_dim),
            nn.Softmax(dim=-1)
        )
    def forward(self, state):
        dist = Categorical(self.actorNN(state))
        return dist

class CriticNN(nn.Module):
    def __init__(self, input_dim, output_dim, hiden_layer = 256):
        super(CriticNN, self).__init__()
        self.criticNN = nn.Sequential(
            nn.Linear(input_dim, hiden_layer),
            nn.ReLU(),
            nn.Linear(hiden_layer, 1)
        )
    def forward(self, state):
        value = self.criticNN(state)
        return value

class Agent:
    def __init__(self, input_dim, output_dim, device):
        self.device = device
        #==========================#
        self.gamma = torch.tensor(0.99).float().to(self.device)
        self.lr_actor = torch.tensor(3e-4).float().to(self.device)
        self.lr_critic = torch.tensor(5e-4).float().to(self.device)
        #==========================#
        self.actor = ActorNN(input_dim, output_dim).to(self.device)
        self.critic = CriticNN(input_dim, output_dim).to(self.device)
        self.target = copy.deepcopy(self.critic)

        self.optim_actor = torch.optim.Adam(self.actor.parameters(), lr=self.lr_actor)
        self.optim_critic = torch.optim.Adam(self.critic.parameters(), lr=self.lr_critic)
```

#### 测试与训练函数 （部分）

```python
def test(agent, device, is_sample = True):
    env_test = gym.make("CartPole-v1", render_mode="human") # gym>=0.25.0
    s, _ = env_test.reset()
    done = False
    while not done:
        dist = agent.actor(torch.as_tensor(s, dtype=torch.float32).to(device))
        if is_sample:
            a = dist.sample().item()
        else:
            a = torch.argmax(dist.probs).item()
        s, _, done, _, _ = env_test.step(a)
        # env_test.render() # 不需要手动render
    env_test.close()


def train(env_train, input_dim, output_dim, is_test, device):
    agent = Agent(input_dim=input_dim, output_dim=output_dim, device=device)

    #=============================#
    n_episodes = 2000 # 5000
    PRINT_FREQUENCY = 20
    UPDATE_FREQUENCY = 10 # 20
    TARGET_UPDATE_FREQUENCY = 50
    #=============================#

    reward_episode_list = []
    reward_ep20avg_list = []
    best_model_parameters = copy.deepcopy(agent.actor.state_dict())
    max_reward = 0
    update_count = 0
    for episode_i in range(1, n_episodes+1):
        s, _ = env_train.reset()
        done = False
        step_i = 0
        #episode_reward = 0
        entropy = 0

        reward_list = []
        reward_update = []
        while not done:
            step_i += 1
            s = torch.as_tensor(s, dtype=torch.float32).to(device)
            if not reward_update: s_0 = s

            # 前向，获取动作
            dist = agent.actor(s)
            a = dist.sample() # <Categorical>对象，dist拥有很多<Attributes> 如dist.probs查看概率 / a为tensor([*])
            if not reward_update: a_0 = a
            # interact
            s_, r, done, _ , _ = env_train.step(a.item())

            r = torch.tensor([r],dtype=torch.float).unsqueeze(-1).to(device)
            reward_update.append(r)
            s = s_
            # if step_i >= 2000: done = True # 手动结束

            if step_i % UPDATE_FREQUENCY == 0 or done:
                reward_list.extend(reward_update)
                reward_till_step_i = torch.cat(reward_list).detach().sum().cpu()

                # 更新
                value = agent.critic(s_0)
                next_value = agent.target(torch.as_tensor(s_, dtype=torch.float32).to(device))
                log_prob = dist.log_prob(a_0).unsqueeze(-1)

                return_k_step = [(agent.gamma**i) * num for i,num in enumerate(reward_update)] # 为每一步奖励加折扣
                return_k_step = torch.cat(return_k_step)
                mask = torch.tensor([1-done],dtype=torch.float).unsqueeze(-1).to(device)
                advantage = return_k_step.sum() + agent.gamma**len(reward_update) * next_value * mask - value

                critic_loss = 0.5 * advantage.pow(2)
                actor_loss = -log_prob * advantage.detach() # 策略网络损失

                agent.optim_critic.zero_grad()
                critic_loss.backward()
                agent.optim_critic.step()

                agent.optim_actor.zero_grad()
                actor_loss.backward()
                agent.optim_actor.step()

                reward_update.clear()
                update_count +=1
                if update_count % TARGET_UPDATE_FREQUENCY == 0:
                    agent.target.load_state_dict(agent.critic.state_dict())

                # 统计、打印
                if done:
                    reward_episode_list.append(reward_till_step_i)
                    if episode_i % PRINT_FREQUENCY == 0:
                        reward_ep20avg_list.append(np.mean(reward_episode_list[-PRINT_FREQUENCY:]))
                        print("Episode: {}, Avg. Reward: {}".format(episode_i, reward_ep20avg_list[-1]))                    
```

#### 运行

```python
if __name__ == "__main__":
    env = gym.make('CartPole-v1')
    input_dim = env.observation_space.shape[0]
    output_dim = env.action_space.n
    #==========================#
    is_load_model = False # 训练模式
    is_load_model = True # 注释则训练，取消注释则加载已有模型

    is_test = True
    #==========================#

    if is_load_model: # 是否从指定路径中加载模型参数
        agent = Agent(input_dim, output_dim, device)
        env.close()
        agent.load_pretrained_model(model_path="./Models/cartpole-multi-step-endless.pth")
        if is_test:
            test(agent, device, is_sample = True)

    else:
        train(env, input_dim, output_dim, is_test=False, device='cpu')
```

#### 训练结果

- 每20回合对最近20回合，以及对过去所有回合累计奖励求平均

<img src="https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/18/20240318-232413.png" alt="A2C_multi-step" style="zoom:100%;" />

- 保存模型测试

![CartPole_v1_result_A2C](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/18/20240318-232651.gif)

<mark>能在随机采样动作的情况下不倒</mark>

#### 关于模型保存

因为一回合要进行多步更新，无法像DQN或是REINFORCE一样通过比较单回合的累计奖励来保存最好的模型。尝试保存最大累计奖励回合倒数第二次更新前的模型，但效果不甚理想（因为导致中断很可能是后面的更新让参数变得不再好）。

在调整超参数`UPDATE_FREQUENCY`时，将原本的10调成20 (即每次更新用到19个奖励)，再增大训练回合，发现训练时会突然"卡主"，回合无法中断，如下

![image-20240318170821267](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/18/20240318-170821.png)

猜想可能是模型突然收敛，且在不断更新的策略中仍能保持环境运行，于是在代码中添加主动return并保存模型。再用保存的模型测试，有了上述结果。

## 总结

- 普通的REINFORCE算法的近似梯度：$\hat {\boldsymbol{g}}(s_t,a_t;\boldsymbol{\theta})\triangleq u_t\cdot\nabla_{\boldsymbol{\theta}}\ln\pi(a_t|s_t;\boldsymbol{\theta}).$
- 普通Actor-Critic算法的近似梯度：$\hat {\boldsymbol{g}}(s_t,a_t;\boldsymbol{\theta})\triangleq q(s_t,a_t;w)\cdot\nabla_{\boldsymbol{\theta}}\ln\pi(a_t|s_t;\boldsymbol{\theta}).$
- 带基线的RINFORCE算法的近似梯度：$\hat {\boldsymbol{g}}(s_t,a_t;\boldsymbol{\theta})\triangleq \left[u_t-v(s_t;w)\right]\cdot\nabla_{\boldsymbol{\theta}}\ln\pi(a_t|s_t;\boldsymbol{\theta}).$
- 带基线的Actor-Critic算法的近似梯度：
  - 单步TD：$\hat g(s_t,a_t;\theta)\triangleq[r_t+\gamma\cdot v(s_{t+1};w)-v(s_t;w)]\cdot \nabla _ { \boldsymbol { \theta }}\ln\pi(a_t\mid s_t;\boldsymbol{\theta})$ **或** $\delta_t^{(1)}\cdot\nabla _ { \boldsymbol { \theta }}\ln\pi(a_t\mid s_t;\boldsymbol{\theta})$
  - 多步(k-step)TD：$\hat g(s_t,a_t;\theta)\triangleq[\sum^{k-1}_{i=0}\gamma^ir_{t+i}+\gamma^k\cdot v(s_{t+k};w)-v(s_t;w)]\cdot \nabla _ { \boldsymbol { \theta }}\ln\pi(a_t\mid s_t;\boldsymbol{\theta})$ **或** $\delta_t^{(k)}\cdot\nabla _ { \boldsymbol { \theta }}\ln\pi(a_t\mid s_t;\boldsymbol{\theta})$

> "REINFORCE is a Monte Carlo algorithm and is well defined only for the episodic case with all updates made in retrospect after the episode is completed"
> 
> REINFORCE算法是一种MC算法，它使用complete return ($u_t$)来近似计算策略梯度；
> 
> Actor-Critic算法则是使用single-step 或 multi-step TD error来近似计算策略梯度；进一步可以发现，在多步TD方法中，若$k=1$，则其和单步TD算法一致，若$k=n$时，则其和带基线的REINFORCE算法趋同 (还是有区别)。

![image-20240311171358812](https://raw.githubusercontent.com/xiyanzzz/Picture-store/main/MarkText_pic/2024/03/11/20240311-171359.png)

<center><p>Image taken from CMU CS10703 lecture slides</p></center>

### 参考链接：

- <https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f>
- [CSDN Actor-Critic及Advantage Actor-Critic(A2C)原理及实战讲解](https://blog.csdn.net/M3197783956/article/details/135014753?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EYuanLiJiHua%7EPosition-2-135014753-blog-127143122.235%5Ev43%5Econtrol&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EYuanLiJiHua%7EPosition-2-135014753-blog-127143122.235%5Ev43%5Econtrol&utm_relevant_index=5)
- <https://blog.csdn.net/weixin_45985148/article/details/127143122>
- [OpenAI gym:将gym运行过程保存为gif](https://blog.csdn.net/ice_bear221/article/details/123735643)

完整代码实现：<https://github.com/xiyanzzz/RL-Implement/tree/main/Policy-based>
